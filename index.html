<!DOCTYPE html>
<html lang="en-us">
	<head>
		<meta charset="UTF-8">
		<title>TRANS-KBLSTM</title>
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="theme-color" content="#157879">
		<link rel="stylesheet" href="css/normalize.css">
		<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
		<link rel="stylesheet" href="css/cayman.css">
	</head>
	<body>
		<section class="page-header" style="background-image: linear-gradient(to right, #272b33 , #58A4B0);">
			<h1><img src="figures/trans-kblstm.png" style="max-width:50%;"></h1>
			<a href="https://aclanthology.org/2022.deelio-1.7/" class="btn">Paper</a>
			<a href="https://github.com/Varun221/trans-kblstm" class="btn">Code</a>
			<a href="https://infotabs.github.io/" class="btn">InfoTabS</a>
			<a href="https://youtu.be/CldBchM8IK0" class="btn">Video</a>
			<a href="https://vgupta123.github.io/docs/Trans-KBLSTM_Poster.pdf" class="btn">Poster</a>
			<a href="https://vgupta123.github.io/docs/Trans-KBLSTM_PPT.pdf" class="btn">PPT</a>
			<a href="https://tabpert.github.io" class="btn">TabPert</a>
		</section>
		<section class="main-content">
			<h1 style="color: #C84630"> <b>TRANS-KBLSTM </b>: An External Knowledge Enhanced Transformer BiLSTM model for Tabular Reasoning</h1>
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>About</h2><p style="text-align: justify;"> Natural language inference on tabular data is a challenging task. Existing approaches lack the world and common sense knowledge required to perform at a human level. While massive amounts of KG data exist, approaches to integrate them with deep learning models to enhance tabular reasoning are uncommon. In this paper, we investigate a new approach using BiLSTMs to incorporate knowledge effectively into language models. <br> <br> Through extensive analysis, we show that our proposed architecture,	Trans-KBLSTM improves the benchmark performance on <a href="infotabs.github.io">INFOTABS </a> , a tabular NLI dataset. <p>
		
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Tabular Inference Problem</h2>
			<p style="text-align: justify; display:inline;"> Given a premise table, the task is to determine whether given hypothesis is true (<span style="color: darkgreen">entailment</span>), false (<span style="color: red">contradiction</span>), or undetermined (<span style="color: blue">neutral</span>, i.e. tabular natural language inference. Below is an example from the <a href="infotabs.github.io">INFOTABS </a>dataset:</p>
			<p style="margin-left:10%; margin-right:10%;"><img src="figures/example.png" style="max-width:95%;"></p>
			<p> Here, <b>H1</b> is <span style="color: darkgreen">entailed</span>, <b>H2</b> is <span style="color: red">contradiction</span> and <b>H3</b> is <span style="color: blue">neutral</span></p>
			
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why Knowledge?</h2>
			<p style="margin-left:10%; margin-right:10%;"><img src="figures/knowledge_example.png" style="max-width:75%;"></p>
			<p style="text-align: justify;"> Predicting the Gold label correctly requires broad understanding of <i> California </i> is located on the <i> Coast </i>
			</ul></p>
			
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Challenges and Motivation</h2>
			<p style="text-align: justify;"> The following are the key challenges encountered while working on any tabular reasoning problem:
			<ul>
				<li>Knowledge Extraction</li>
				<li>Knowledge Representation</li>
				<li>Knowledge Integration </li>
			</ul>
		    </p>
		    <h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a><span>&#9672;</span> Knowledge Extraction</h2>
		    <h3><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Challenge</h3>
		    <p style="text-align: justify;"> KG Explicit (from <a href="https://knowledge-infotabs.github.io/">KNOWLEDGE_INFOTABS</a>) augments the input with <b> With lengthy key definitions </b> that are susceptible to noise and spurious correlations. </p> 
		    <h3><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Solution: Relational Connections</h3>
			<p style="margin-left:10%; margin-right:10%;"><img src="figures/relationalconn.png" style="max-width:85%;"></p>
		    <p style="text-align: justify;"> <b>Semantic Knowledge Graphs</b> represent the relationships between hypothesis and premise token pairs. <p>
		    <p style="text-align: justify;"> To extract relevant knowledge, We use the semantic relational connections between premise and hypothesis tokens. </p>

		    <h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a><span>&#9672;</span> Knowledge Representation</h2>
		    <h3><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Challenge</h3>
			<p style="margin-left:10%; margin-right:10%;"><img src="figures/rep_challenge.png" style="max-width:85%;"></p>
		    <p style="text-align: justify"> Appending bulky definitions at input introduces unnecessary noise </p>
		    <h3><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Solution: Using Sentence Embeddings</h3>
			<p style="margin-left:10%; margin-right:10%;"><img src="figures/rep_sol.png" style="max-width:85%;"></p>
		    <p style="text-align: justify"> Knowledge triples are first converted to sentences (refer to <a href="https://vgupta123.github.io/docs/TransKBLSTM.pdf">PAPER</a> for more details) and then embedded using <b> Sentence Transformers </b> </p>



			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a><span>&#9672;</span> Knowledge Integration</h2>
		    <h3><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Challenge</h3>
		    <p style="text-align: justify;"> The main challenge that this work tries to address is the integration of external knowledge word pairs into transformer architecture. </p> 
		    <p style="text-align: justify;"> <br> Consider a word pair relation between <i>California</i> and <i>Coast</i> from <a href="https://conceptnet.io">Conceptnet</a></p> 
			<p style="margin-left:10%; margin-right:10%;"><img src="figures/relation.png" style="max-width:70%;"></p>
		    <p style="text-align: justify;"> If we try to add this relation to transformer, the tokenizer will break apart <i>California</i> into <i>Cal if ornia</i></p> 
			<p style="margin-left:10%; margin-right:10%;"><img src="figures/intebad.png" style="max-width:70%;"></p>
		    <h3><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Solution: Use BiLSTM Models</h3>
		    <p style="text-align: justify;"> BiLSTMs use word level embeddings, hence can easily add word pair relations. In our work we use 300 Dimensional <a href="https://nlp.stanford.edu/projects/glove/">Glove Embeddings</a></p> 
			<p style="margin-left:10%; margin-right:10%;"><img src="figures/integood.png" style="max-width:70%;"></p>

			
		

		    <h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Solution Pipeline</h2>
		    <p style="text-align: justify; display:inline;"> With the solutions prescribed above, we develop the architecture <b>TRANS-KBLSTM</b> </p>
			<p style="margin-left:10%; margin-right:10%;"><img src="figures/model.png" style="max-width:95%;"></p>
		    <p style="text-align: justify; display:inline;"> Our full solution pipeline consists of - </p>
			<ul>
				<li>Relational connection retrieval from ConceptNet and Wordnet</li>
				<li>Converting to Phrases and Encoding using Sentence Transformers</li>
				<li>Generation of Relational Attention and embedding matrices</li>
				<li>Using BiLSTM encoders to Encode Premise and Hypothesis</li>
				<li>Multi-Head dot product attention to weight the importance of external knowledge into premise and hypothesis context</li>
				<li>Compose knowledge using attention weights obtained from previous step</li>
				<li>Mean and Max Pool the premise and hypothesis composed vectors</li>
				<li>Combine the pooled embeddings with transformer embeddings</li>
				<li>Apply Regularization and classify into 3 classes</li>
			</ul>
		    <p style="text-align: justify; display:inline;"> For a more detailed description, please refer to the <a href="https://vgupta123.github.io/docs/TransKBLSTM.pdf">PAPER</a></p>



		    <h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experimental Results</h2>
			<p style="text-align: justify; display:inline;">We observe improvements over pre-established baselines on all test sets of INFOTABS. <br><br> <img src="figures/fullres.png" style="max-width:95%;"></p>
			<p style="text-align: justify; display:inline;"><br> <br> We observe significant improvements on limited supervision. <br><br> <img src="figures/limitedres.png" style="max-width:95%;"></p>
			<p style="text-align: justify; display:inline;"><br> <br> We also observe improvements across different reasoning types <br><br> <img src="figures/reasoning.png" style="max-width:95%;"></p>
			<!-- <p style="text-align: justify; display:inline;"> <br> Ablation Studies <br><br> <img src="figures/ablation.png" style="max-width:95%;"></p>
			 -->

			 <h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ablation Studies</h2>
			<p style="text-align: justify; display:inline;">We remove the <b>Embedding mix-skip connection</b> and also introduce noise in place of knowledge to observe the decrement. We notice that removing knowledge in &alpha;2 and &alpha;3 test sets of INFOTABS <br><br> <img src="figures/removeskip.png" style="max-width:95%;"></p>
			<p style="text-align: justify; display:inline;"> <br> <br> We also explore the results on <b>Joint</b> and <b>Independent</b> training where we first train Transformer and then train the BiLSTM encoder with transformer weights freezed. <br><br> <img src="figures/jointrain.png" style="max-width:95%;"></p>

			

		    <p style="text-align: justify; display:inline;"> <br> <br> <br> As always, for a more detailed description, please refer to the <a href="https://vgupta123.github.io/docs/TransKBLSTM.pdf">PAPER</a></p>


			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2><p style="text-align: justify;"> Our proposed architecture <i>TRANS-KBLSTM</i> shows improvements across all test sets of INFOTABS with the increment being more pronounced in low-data regimes. We believe that our findings will be beneficial to researchers working on the integration of external knowledge to deep learning architectures. The described pipeline can be applicable to <i>Question Answering</i> and <i>Dialogue understanding</i> as well.</p> 
					<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>TabPert</h2>
			<p style="text-align: justify;"> You should check our <a href="https://2021.emnlp.org">EMNLP 2021</a> paper which is a <a href="https://tabpert.github.io">tabular perturbation platform</a> to generate counterfactual examples.</p>
	
<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>People</h2>
			<p style="text-align: justify;"> The following people have worked on the paper<a href="https://vgupta123.github.io/docs/TransKBLSTM.pdf"> "TRANS-KBLSTM: An External Knowledge Enhanced Transformer BiLSTM model for Tabular Reasoning"</a>: </p>
			<figure>
				<img src="figures/varun.jpg" style="width:10em;height:15em">
				<img src="figures/aayush.jpg" style="width:10em;height:15em">
				<img src="figures/vivekg.jpg" style="width:10em;height:15em">
				<figcaption>From left to right, <a href="https://www.linkedin.com/in/yvarun25221/">Yerram Varun</a>, <a href="https://www.linkedin.com/in/aayushsharma9753/">Aayush Sharma</a> and <a href="https://vgupta123.github.io">Vivek Gupta</a>. </figcaption>
			</figure>
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Citation</h2>
			<p style="text-align: justify;"> Please cite our paper as below.</p>
			<pre><code>@inproceedings{varun-etal-2022-trans,
    title = "Trans-{KBLSTM}: An External Knowledge Enhanced Transformer {B}i{LSTM} Model for Tabular Reasoning",
    author = "Varun, Yerram  and
      Sharma, Aayush  and
      Gupta, Vivek",
    booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
    month = may,
    year = "2022",
    address = "Dublin, Ireland and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.deelio-1.7",
    pages = "62--78",
    abstract = "Natural language inference on tabular data is a challenging task. Existing approaches lack the world and common sense knowledge required to perform at a human level. While massive amounts of KG data exist, approaches to integrate them with deep learning models to enhance tabular reasoning are uncommon. In this paper, we investigate a new approach using BiLSTMs to incorporate knowledge effectively into language models. Through extensive analysis, we show that our proposed architecture, Trans-KBLSTM improves the benchmark performance on InfoTabS, a tabular NLI dataset.",
}</code></pre> 
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Acknowledgement</h2>
			<p style="text-align: justify;">Authors thank members of the <a href="https://svivek.com/">Utah NLP group</a> for their valuable insights and
			suggestions at various stages of the project; and <a href="https://sites.google.com/view/deelio-ws/">DeeLIO Workshop</a> reviewers for their helpful comments. Additionally, we appreciate the inputs provided by <a href="https://svivek.com/">Vivek Srikumar</a> and <a href="http://www.cs.utah.edu/~riloff/">Ellen Riloff</a>.  <a href="https://vgupta123.github.io">Vivek Gupta</a> acknowledges support from Bloomberg's
			Data Science Ph.D. Fellowship.</p>
			<footer class="site-footer">
				<span class="site-footer-owner"><a href="https://trans-kblstm.github.io">TRANS-KBLSTM</a> is maintained by <a href="https://www.linkedin.com/in/yvarun25221/">Yerram Varun</a>.</span>
				<span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman</a> theme by <a href="https://github.com/jasonlong">jasonlong</a>.</span>
			</footer>
		</section>
	</body>
</html>
